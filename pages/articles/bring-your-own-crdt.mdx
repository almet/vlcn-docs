# Bring your Own CRDT

I recently watched https://twitter.com/aboodman give a talk about Replicache. I loved seeing the Replicache model and the talk gave me a bit of envy. Envious of how simple new semantics can be built out without having to understand CRDTs.

So I was wondering, can we bring the same developer experience to CRDTs?

## Background

Replicache has a model where developers create named mutations.

E.g.,

```js
export mutations {
  increment(tx, amount) {
    const current = tx.get('count');
    const next = current + amount;
    tx.put('count', next);
  }
}
```

These named mutations exist both on the server and client. The client can optimistically apply mutations to its local state as often as it likes. In the background, these mutations are shipped off to the server as a tuple of [mutation_name, ...args]. The server then merges changes from all clients by coming up with a total ordering of mutations and applying them in that order. There are a few more details such that the server can never re-wind its state to incorporate new operations (only clients can do that), but that's the gist of the Replicache model.

This model is great. Developers can write arbitrary logic in their mutations and still end up with convergent state. This lets devs work with what they're familiar with (normal code) rather than having to reach for and learn about the correct CRDT to model their data.

One tradeoff of the Replicache approach, however, is that all changes must go through a central server. It can't work in a P2P setting.

But it got me thinking, can we bring the Replicache model to CRDTs? Can we give developers a way to write arbitrary logic and still guarantee convergence of their state in both client-server and P2P settings?

## Bring your own CRDT

Yes we can! And I'm calling it "bring your own CRDT". It is a re-think on my earlier work ([LWW vs DAG](./lww-vs-dag)) to incoroprate named mutations (part inspired by Replicache part from [another earlier work of mine](https://aphrodite.sh/docs/mutations-and-transactions)) that exist on all peers. I.e., a re-think of my earier work in the light of Replicache, extended to work outside of client-server configurations.

## Building the Event Log

The basis of the model is an event log. Just like Replicache, we'll model the event log as `[mutation_name, ...args]` tuples. One thing we'll add to each event, however, is a pointer back to the event which preceeded it. This pointer will allow us to build a tree of events where forks represent concurrent edits. A deterministic ordering of this tree will create a total ordering of events and allow us to converge on a single state.

```sql
CREATE TABLE event (id INTEGER PRIMARY KEY, mutation_name TEXT, args ANY) STRICT;
CREATE TABLE event_dag (
  parent_id ANY, -- the event that came before this one
  event_id INTEGER, -- the id of the event
  PRIMARY KEY (parent_id, event_id)
) STRICT;
CREATE INDEX event_dag_event ON event_dag (event_id);
```

## Declaring Mutations

Next, we need to require that users declare mutations. These mutations must exist on all peers. Mutations must be deterministic but they need not be idempotent as we will guarantee that the effects of a mutation on local state will only be observed once.

> Note: can't the user shoot themselves in the foot if their mutation changes some globals? Yes but I think this is an acceptable risk. If it is not, we define mutations in [AssemblyScript](https://www.assemblyscript.org/). This allows us to completely sandbox the mutation code and _only_ give it access to what is safe to read and write. `AssemblyScript` is another super power given we can compile it to WASM and run this WASM code in the database itself rather than requiring the user to spin up a separate backend.

```js
export mutations {
  addTodo(tx, amount) {
  },
  removeTodo() {},
  completeTodo() {},
  renameTodo() {},
  completeAllTodos() {},
  uncompleteAllTodos() {},
  clearCompletedTodos() {}
}
```